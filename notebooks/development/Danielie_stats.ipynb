{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import sys\n",
    "sys.path.insert(0, '/Users/jsmonzon/Research/SatGen/mcmc/src/')\n",
    "import jsm_halopull\n",
    "import jsm_SHMR\n",
    "import jsm_mcmc\n",
    "import jsm_stats\n",
    "import jsm_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Danieli et al 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They have 27 host galaxies in their local volume sample. They select the host halo mass of each galaxy 50 times to account for the scatter at fixed stellar mass (assuming 0.15 dex). This gives them 50 local volume realizations, similar to Nadler's 45 MW zoom-in simulations.\n",
    "\n",
    "Importantly they, only have 280 unique merger trees (280 unique accretion histories) to sample from. We want to know how often they reuse each merger tree when selecting their 27x50 hosts. Since we don't know the stellar masses of each ELVES host galaxy, we can approximate thier sample by tring to match Figure 3 in their paper. I found that a Gaussian distribution centered on Mhost = 12.3 with a width of 0.23 dex qualitatively matches their local volume mass function. Below you can see 50 draws from that Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binz = np.linspace(11.6, 13.1, 25) # in log halo mass\n",
    "plt.title(\"local volume mass function\")\n",
    "for i in range(50): # just to match the figure\n",
    "    data, binr = jsm_stats.cumulative(np.random.normal(loc=12.3, scale=0.23, size=(27)), mass_bins=binz, return_bins=True)\n",
    "    plt.plot(10**binr, data, color=\"grey\", alpha=0.3)\n",
    "plt.xscale(\"log\")\n",
    "plt.ylabel(\"N(>Mhost)\")\n",
    "plt.xlabel(\"log Mhost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So lets just start of with a single 27x50 sample..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Danieli_binz = np.linspace(10.5, 13.3, 281) # so that their are 280 bincenters\n",
    "\n",
    "single_sample = np.random.normal(loc=12.3, scale=0.23, size=(27*50)) # this is their full sample\n",
    "\n",
    "counts = np.histogram(single_sample, bins=Danieli_binz)\n",
    "\n",
    "bin_centers = (counts[1][:-1] + counts[1][1:]) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts[0] # these are counts for the 280 bins!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nbins_used = sum(counts[0] >= 1) # lets just count how many of the 280 bins got a hit, Ill call this a unique merger tree!\n",
    "Nbins_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_index = np.where(counts[0] == max(counts[0]))[0][0]\n",
    "plt.title(\"single sample\")\n",
    "plt.axvline(bin_centers[max_index], color=\"red\", ls=\"--\", lw=0.5, label=f\"{bin_centers[max_index]:.2f}\")\n",
    "plt.step(bin_centers, counts[0], lw=0.5, color=\"black\")\n",
    "plt.xlabel(\"log Mhost\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 128 of the 280 bins were selected in this particular draw, with the vast majority being above ~11.5 in halo mass.\n",
    "\n",
    "The log Mhost = 12.25 bin happened to be the one with the most counts: 32 out of 1350 (roughly 2% of the sample).\n",
    "\n",
    "Lets do this several times and see what we find..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nrepeate = 10000\n",
    "\n",
    "multi_sample = np.random.normal(loc=12.3, scale=0.23, size=(Nrepeate, 27*50)) # lets run it 10000 times!\n",
    "\n",
    "counts_mat = np.zeros(shape=(Nrepeate, bin_centers.shape[0])) # just to store the histograms\n",
    "for i,vals in enumerate(multi_sample):\n",
    "    counts_mat[i] = np.histogram(vals, bins=Danieli_binz)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"several samples\")\n",
    "for i in range(10): # 10 just to visualize the scatter\n",
    "    plt.step(bin_centers, counts_mat[i], lw=0.5, color=\"black\", alpha = 0.2)\n",
    "plt.xlabel(\"log Mhost\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets see on average how many unique merger trees are sampled..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = np.sum(counts_mat >= 1, axis=1) # this averages over the Nrepeate=10000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(unique, bins=15, edgecolor=\"white\")\n",
    "plt.axvline(np.average(unique), color=\"red\", ls=\"--\", label=f\"{np.average(unique):.2f}\")\n",
    "plt.xlabel(\"N unique merger trees used\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So on average only ~124 unique merger trees out of 280 are used in their modeling!\n",
    "\n",
    "That is ~45% of their sample! Importantly, the vast majority of these 124 trees are being selected more than once!\n",
    "\n",
    "lets normalize the histogram from before to see what percentage of the 1350 samples fall into each bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"several samples\")\n",
    "for i in range(10): # 10 just to visualize the scatter\n",
    "    plt.step(bin_centers, 100*(counts_mat[i]/(1350)), lw=0.5, color=\"black\", alpha = 0.2)\n",
    "plt.xlabel(\"log Mhost\")\n",
    "plt.ylabel(\"% of sample\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can that roughly 2% of the 1350 selections fall in the ~40 bins between 12.1 < log Mhost < 12.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now lets play with scatter at fixed stellar mass, since after all we are just assuming its 0.23 dex...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samplesize_test(loc, sigma, Nbinz, Ndanieli):\n",
    "    Nrepeate = 100\n",
    "    lgMh_draws = np.random.normal(loc=loc, scale=sigma, size=((Nrepeate, Ndanieli)))\n",
    "\n",
    "    lgMh_grid = np.linspace(10.5, 13.3, Nbinz)\n",
    "    \n",
    "    counts = np.zeros(shape=(Nrepeate, Nbinz-1))\n",
    "    for i,vals in enumerate(lgMh_draws):\n",
    "        counts[i] = np.histogram(vals, bins=lgMh_grid)[0]\n",
    "\n",
    "    Nbins_used = np.sum(counts > 0.0, axis=1)\n",
    "\n",
    "    return np.array([np.average(Nbins_used), np.std(Nbins_used)]), counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nsigma = 50\n",
    "scatter = np.linspace(0.0, 0.5, Nsigma)\n",
    "Nbins_used_more_than = np.zeros(shape=(Nsigma, 2))\n",
    "Nbins_used = np.zeros(shape=(Nsigma, 2))\n",
    "Nsamples = 27*50\n",
    "Nbins = 280\n",
    "\n",
    "for i,sigma in enumerate(scatter):\n",
    "    Nbins_used_i, test = samplesize_test(12.3, sigma, Nbinz=Nbins, Ndanieli=Nsamples)\n",
    "    Nbins_used[i] = Nbins_used_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(scatter, (Nbins_used[:,0]/Nbins)*100, yerr=(Nbins_used[:,1]/Nbins)*100, capsize=3, fmt=\".\", color=\"k\")\n",
    "plt.xlabel(\"$\\\\sigma_{M_{host}}$\", fontsize=20)\n",
    "plt.ylabel(\"% unique merger trees sampled\")\n",
    "plt.axvline(0.23, ls=\"--\")\n",
    "plt.ylim(0,100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = np.load(\"../../../data/MW-analog/meta_data_psi3/Danieli-stats/model_1/models.npz\")\n",
    "main_path = \"../../../data/MW-analog/meta_data_psi3/Danieli-stats/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass2 = full[\"mass\"][0:10]\n",
    "red2 = full[\"redshift\"][0:10]\n",
    "path2 = \"model_2/\"\n",
    "\n",
    "np.savez(main_path+path2+\"models.npz\",\n",
    "        mass = mass2,\n",
    "        redshift = red2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass3 = full[\"mass\"][0:5]\n",
    "red3 = full[\"redshift\"][0:5]\n",
    "path3 = \"model_3/\"\n",
    "\n",
    "np.savez(main_path+path3+\"models.npz\",\n",
    "        mass = mass3,\n",
    "        redshift = red3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass4 = full[\"mass\"][0:3]\n",
    "red4 = full[\"redshift\"][0:3]\n",
    "path4 = \"model_4/\"\n",
    "\n",
    "np.savez(main_path+path4+\"models.npz\",\n",
    "        mass = mass4,\n",
    "        redshift = red4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import sys\n",
    "sys.path.insert(0, '/Users/jsmonzon/Research/SatGen/mcmc/src/')\n",
    "import jsm_halopull\n",
    "import jsm_SHMR\n",
    "import jsm_mcmc\n",
    "import jsm_stats\n",
    "import jsm_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed = [False, True, False, False, True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiducial_s0 = jsm_mcmc.Chain(\"../../mcmc/inference_tests/s0/mock_jsm/chain.h5\", fixed=fixed, Nstack=500, Nburn=100, Nthin=30)\n",
    "\n",
    "nadler_s0 = jsm_mcmc.Chain(\"../../mcmc/inference_tests/s0/mock_nadler/chain.h5\", fixed=fixed, Nstack=500, Nburn=100, Nthin=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiducial_s15 = jsm_mcmc.Chain(\"../../mcmc/inference_tests/s15/mock_jsm/chain.h5\", fixed=fixed, Nstack=500, Nburn=100, Nthin=30)\n",
    "\n",
    "nadler_s15 = jsm_mcmc.Chain(\"../../mcmc/inference_tests/s15/mock_nadler/chain.h5\", fixed=fixed, Nstack=500, Nburn=100, Nthin=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiducial_s30 = jsm_mcmc.Chain(\"../../mcmc/inference_tests/s30/mock_jsm/chain.h5\", fixed=fixed, Nstack=500, Nburn=100, Nthin=30)\n",
    "\n",
    "nadler_s30 = jsm_mcmc.Chain(\"../../mcmc/inference_tests/s30/mock_nadler/chain.h5\", fixed=fixed, Nstack=500, Nburn=100, Nthin=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_labels = [\"our method \\n $\\\\alpha = $\" + fiducial_s0.constraints[0] + \"\\n $\\\\sigma = $\" + fiducial_s0.constraints[1],\n",
    "                \"Danieli's method \\n $\\\\alpha = $\" + nadler_s0.constraints[0] + \"\\n $\\\\sigma = $\" + nadler_s0.constraints[1]]\n",
    "\n",
    "data_s0 = [fiducial_s0.clean, nadler_s0.clean]\n",
    "fid_values = [2.0, 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = jsm_mcmc.MulitChain(data, chain_labels, fixed)\n",
    "test.plot_posteriors(filledPlots=True, nContourLevels=3, smoothingKernel=1.5, truths=fid_values)#, colorsOrder=[\"blues\", \"oranges\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subhalos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
